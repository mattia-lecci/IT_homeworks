\mysection{5.20}{Problem 5.20}

We consider the three-sender DM-MAC described by the exercise. We have two message sets [1:$2^{nR_1}$] and [1:$2^{nR_2}$], one encoder that assigns $x^n(m_1)$ to each message $m_1 \in [1:2^{nR_1}]$, an encoder that assigns $x^n(m_2)$ to each message $m_2 \in [1:2^{nR_2}]$ and an encoder that assign $x_3^n(m_1,m_2)$ to each pair $(m_1,m_2) \in [1:2^{nR_1}] \times [1:2^{nR_2}]$. We notice that messages $m_1$ and $m_2$ are considered to be independent. The three encoded messages are sent trough a MAC channel. Then a decoder assigns an estimate $(\hat{m_1},\hat{m_2}) \in [1:2^{nR_1}] \times [1:2^{nR_2}]$ to each received sequence. We define the average probability error as in \eqref{eq:averror}.

\begin{equation}
	\Pr(\E) \triangleq\Pr((\hat{m_1},\hat{m_2}) \neq (m_1,m_2))
	\label{eq:averror}
\end{equation}

We want to show that the capacity region $C(R_1, R_2)$ of the system is given by the set of $(R_1,R_2)$ contained in \eqref{eq:cregion} for some pdf $p(q,x_1,x_2,x_3)=p(q)p(x_1|q)p(x_2|q)p(x_3|x_1,x_2,q)$

\begin{equation}
	\begin{cases}
		R_1 &\leq I(X_1,X_3;Y|X_2,Q) \\
		R_2 &\leq I(X_2,X_3;Y|X_1,Q) \\
		R_1+R_2 &\leq I(X_1,X_2,X_3;Y|Q)
	\end{cases}
	\label{eq:cregion}
\end{equation}

\subsection{Achievability}

We randomly and independently generate $2^{nR_1}$ sequences $x_1^n(m_1)$ according to a chosen $p(x_1)$, $2^{nR_2}$ sequences $x_2^n(m_2)$ according to a chosen $p(x_2)$ and $2^{n(R_1+R_2)}$ sequences $x_3^n(m_1,m_2)$ according to a chosen $p(x_3|x_1,x_2)$. The described codebook is revealed to all the encoders and the decoder. In order to send the pair $(m_1,m_2)$, the first encoder transmits $x_1^n(m_1)$, the second encoder transmits $x_2^n(m_2)$ while the third encoder transmits $x_3^n(m_1,m_2)$.

We use a \textit{simultaneous decoding} strategy. Therefore the decoder declares that $(\hat{m_1},\hat{m_2})$ was sent if it is the unique pair that satisfies \eqref{eq:sentcond}, otherwise it declares an error.

\begin{equation}
	(x_1^n(\hat{m_1}),x_2^n(\hat{m_2}), x_3^n(\hat{m_1}, \hat{m_2}), y^n) \in \typicalitySet{}
	\label{eq:sentcond}
\end{equation}

First we bound the error event averaged over all the possible random codebooks and messages, obtaining

\begin{equation}
	\Pr(\E)=\Pr(\E | M_1=1,M_2=1)
	\label{eq:boundep}
\end{equation}

Given $(M_1=1,M_2=1)$ the triple $(X_1^n(m_1),X_2^n(m_2), X_3^n(m_1,m_2),Y^n)$, the random nature of the codebook induces the following pdf $p(x_1^n,x_2^n,x_3^n,y^n)$.

\begin{center}
    \begin{tabular}{ | c | c | c |}
    \hline
		$m_1$ & $m_2$ & $p(x_1^n,x_2^n,x_3^n,y^n)$ \\ \hline
		1 & 1 & $p(x_1^n)p(x_2^n)p(x_3^n|x_1^n,x_2^n)p(y^n|x_1^n,x_2^n,x_3^n)$ \\ \hline
    1 & $\neq 1$ & $p(x_1^n)p(x_2^n)p(x_3^n)p(y^n|x_1^n)$ \\ \hline
    $\neq 1$ & 1 & $p(x_1^n)p(x_2^n)p(x_3^n)p(y^n|x_2^n)$ \\ \hline
		$\neq 1$ & $\neq 1$ & $p(x_1^n)p(x_2^n)p(x_3^n)p(y^n)$ \\ \hline
    \end{tabular}
\end{center}

Given $(M_1=1,M_2=1)$ the error event $\E$ can be decomposed as $\E = \E_1 \cup \E_2 \cup \E_3 \cup \E_4$.

\begin{equation}
	\begin{split}
		\E_1 &= \{ (x_1^n(1),x_2^n(1), x_3^n(1,1), y^n) \notin \typicalitySet{} \} \\
		\E_2 &= \{\exists m_1 \neq 1 : (x_1^n(\hat{1}),x_2^n({m_2}), x_3^n(1,m_2), y^n) \in \typicalitySet{} \} \\
		\E_3 &= \{\exists m_1 \neq 1 : (x_1^n({m_1}),x_2^n(1), x_3^n({m_1},1), y^n) \in \typicalitySet{} \} \\
		\E_4 &= \{\exists m_1,m_2 \neq 1 : (x_1^n({m_1}),x_2^n({m_2}), x_3^n({m_1},{m_2} ), y^n) \in \typicalitySet{} \} \\
	\end{split}
\end{equation}

By the union bound we can state that $\Pr(\E) \leq \Pr(\E_1) + \Pr(\E_2) + \Pr(\E_3) + \Pr(\E_4) $. $\Pr(\E)$ will tend to $0$ if $\Pr(\E_i) \rightarrow 0$ $\forall i \in \{1,2,3,4\}$. Therefore we focus on the single error probabilities $\Pr(\E_1)$, $\Pr(\E_2)$, $\Pr(\E_3)$ and $\Pr(\E_4)$.

\begin{itemize}
	\item As repeatedly demonstrated during the course $\Pr(\E_1)$ will tend to 0 as $n \rightarrow \infty$ for the Law of Large Number.

	\item The induced pdf in $\E_2$ is $p(x_1^n)p(x_2^n)p(x_3^n)p(y^n|x_1^n)$ which means that ($X_2^n(m_2)$, $X_3^n(1,m_2)$) is independent from ($X_1^n(1)$, $Y^n$). Therefore we can use the Packing Lemma setting $U \leftarrow \emptyset $, $X \leftarrow (X_2,X_3)$ and $Y \leftarrow (X_1,Y)$. Doing so we obtain what follows.

	\begin{equation*}
	\begin{split}
		\Pr(\E_2) \rightarrow 0 \Leftarrow R_2 &\leq I(X_2,X_3;Y,X_1)\\
		&= I(X_2,X_3;X_1) + I(X_2,X_3;Y|X_1)\\
		&= I(X_2,X_3;Y|X_1)
		\end{split}
	\end{equation*}

	\item The induced pmf in $\E_3$ is $p(x_1^n)p(x_2^n)p(x_3^n)p(y^n|x_2^n)$ which means that ($X_1^n(m_1)$, $X_3^n(m_1,1)$) is independent from ($X_2^n(1)$, $Y^n$). Therefore we can use the Packing Lemma setting $U \leftarrow \emptyset $, $X \leftarrow (X_1,X_3)$ and $Y \leftarrow (X_2,Y)$. Doing so we obtain what follows.

	\begin{equation*}
		\Pr(\E_3) \rightarrow 0 \Leftarrow R_1 \leq I(X_1,X_3;Y|X_2)
	\end{equation*}

	\item The induced pdf in $\E_4$ is $p(x_1^n)p(x_2^n)p(x_3^n)p(y^n)$ which means that ($X_1^n(m_1)$, $X_2^n(m_2)$, $X_3^n(m_1,m_2)$) is independent from $Y^n$. Therefore we can use the Packing Lemma setting $U \leftarrow \emptyset $, $X \leftarrow (X_1, X_2, X_3)$ and $Y \leftarrow Y$. Doing so we obtain what follows.

	\begin{equation*}
		\Pr(\E_4) \rightarrow 0 \Leftarrow R_1+R_2 \leq I(X_1,X_2,X_3;Y)
	\end{equation*}
\end{itemize}

The three limits just found are valid for every possible $p(x_1),p(x_2),p(x_3)$, defining a region that we call $C^1(R_1,R_2)$ (with the usual convex hull and union over all possible probability distributions).

\subsection{Converse}

Let's assume to use a code that ensures $\Pr(\E) \rightarrow 0$ as $n \rightarrow \infty$. The pdf of  $(M_1,M_2,X_1^n,X_2^n,X_3^n,Y^n)$ is shown in \eqref{eq:convpmf}.

\begin{equation}
	p(x_1^n,x_2^n,x_3^n,y^n)\approx2^{-n(R_1+R_2)}p(x_1|m_1)p(x_2|m_2)p(x_3|m_1,m_2)\prod_{i=1}^n p(y_i|x_{1i},x_{2i},x_{3i})
	\label{eq:convpmf}
\end{equation}

By Fano's inequality we can state what follows.

\begin{equation}
	\begin{gathered}
		H(M_1,M_2|Y^n) \leq \log(|M_1\times M_2|)\Pr(\E)+1 = n(R_1+R_2)\Pr(\E)+1 = n\epsilon_n\\
		\Rightarrow
		\begin{cases}
			H(M_1|Y^n,M_2) \leq H(M_1,M_2|Y^n) \leq n\epsilon_n \\
			H(M_2|Y^n,M_1) \leq H(M_1,M_2|Y^n) \leq n\epsilon_n
		\end{cases}
	\end{gathered}
	\label{eq:recallfano}
\end{equation}

We notice that as the error probability $\Pr(\E)$ goes to 0, $n\epsilon_n$ vanishes, too, as $n \rightarrow \infty$. Now we define $Q \sim U[1:n]$ such that $p(q)=\frac{1}{n}$ $\forall q \in [1:n]$. Knowing \eqref{eq:recallfano} we can follow the same procedure applied in chapter 4 for the MAC channel coding converse.

\begin{equation}
	\begin{gathered}
		R_1 \leq \sum_{i=1}^n \Pr(Q=i) I(X_{1i},X_{3i};Yi|X_{2i},Q=i) + n\epsilon_n = I(X_{1},X_{3};Y|X_{2},Q)+n\epsilon_n\\
		R_2 \leq \sum_{i=1}^n \Pr(Q=i) I(X_{2i},X_{3i};Yi|X_{1i}, Q=i) + n\epsilon_n = I(X_{2},X_{3};Y|X_{1},Q)+n\epsilon_n\\
		R_1 + R_2 \leq \sum_{i=1}^n \Pr(Q=i)I(X_{1i},X_{2i},X_{3i};Y_i, Q=i) + n\epsilon_n = I(X_{1},X_{2},X_{3};Y,Q)+n\epsilon_n \\
	\end{gathered}
\end{equation}

The three limits previously found define a region that we call $C^2(R_1,R_2)$.

As proved during the course we can state that $C^1(R_1,R_2)$ and $C^2(R_1,R_2)$ define the same region. We have that $C^1 \subseteq C^2$ since all achievable rates must be contained in $C^2$. We have also that $C^2 \subseteq C^1$ since any pmf $p(q)p(x_1|q)p(x_2|q)p(x_3|x_1,x_2,q)p(y|x_1,x_2,x_3,q)$ define a pentagonal region $C^2$ which corner are contained in $C^1$. Therefore the problem is solved.
