% !TEX root = C:\Users\Famiglia\Documents\GitHub\IT_homeworks\HW2\Solution.tex
\mysection{5.16}{Problem 5.16}
\renewcommand{\arraystretch}{2}
The k-receiver Gaussian Broadcast Channel is composed of k Gaussian BC's, one for each receiver. The output of the channel for the transmitted message $X$ at receiver 1 is $Y_1 = X+Z_1$ and at the $i-th$ receiver is $Y_i = X+Z_i$, where $Z_1,\tilde{Z}_2,...,\tilde{Z}_k$ are Gaussian noise components with powers $N_1,\tilde{N}_2,....,\tilde{N}_k$ respectively. We will assume that the notation is the same as the one used in chapter 5, i.e., the considered BC is degraded and receiver 1 is the strongest. \\
Let $m_i$ be the message destined to the $i$-th receiver, $i=1,2,...,k$.
\subsection{Achievability}
Achievability can be proved using superposition coding.
\subsubsection*{Codebook generation.}
 For each receiver $i$ fix a pmf $p(u_i)p(x|u_i)$. For each $i$, randomly and independently generate $2^{nR_i}$ sequences $u^n_i(m_i)$, $m_i\in [1:2^{nR_i}]$, $U_i\sim \mathcal{N}(0, \alpha_i P)$, with $\alpha_1 = 1-\sum_{h=2}^k \alpha_h$.
According to the superposition coding, to transmit $m_1,m_2,...,m_k$ the transmitter sends
\begin{equation}
X = U_1+ U_2+...+U_k = \sum_{i=1}^k U_i
\end{equation}
We will use the word "message" to denote both the message $m$ and its encoded version $u$ since they play the same role in this dimostration.
\subsubsection*{Encoding}
To send $(m_1,m_2,....,m_k)$ transmit $x^n(m_1,m_2, ...,m_k)$.
\subsubsection*{Decoding}
\begin{itemize}
\item \textbf{@ receiver k} Receiver $k$ receives
\begin{equation}
Y_k = Y_{k-1}+\tilde{Z}_k = .. = U_1 + U_2 + ... +U_k + Z_k
\end{equation}
Applying the \underline{superposition decoding}, receiver $k$ treats $\sum_{h=1}^{k-1} U_h$ as noise, together with $Z_k$. Accordingly, and applying the knowledge of the point-to-point Gaussian channel, we know that the probability of error goes to zero if
\begin{equation}
  R_k<C\left(\frac{S}{N}\right) = C\left(\frac{\alpha_k P}{\left(\sum_{h=1}^{k-1}\alpha_h \right)P + N_k}\right) = C\left(\frac{\alpha_k P}{\left(1-\alpha_k\right)P + N_k}\right)
  \label{eq:k_cond_superpos}
\end{equation}
\item \textbf{@ receiver i} Receiver $i$, $i\neq 1,k$, receives
\begin{equation}
Y_i = Y_{i-1}+\tilde{Z}_i = .. = U_1 + U_2 + ... +U_k + Z_i
\end{equation}
First, $U_k$ is retrieved, using the same procedure as in the previous case.
As before, the probability of error goes to zero if
\begin{equation}
  R_k<C\left(\frac{S}{N}\right) = C\left(\frac{\alpha_k P}{\left(\sum_{h=1}^{k-1}\alpha_h \right)P + N_i}\right) = C\left(\frac{\alpha_k P}{\left(1-\alpha_k\right)P + N_i}\right)
  \label{eq:i_cond_superpos}
\end{equation}
Notice that the only difference with the previous case is the noise term at the denominator. Since $N_k>N_i\ \forall i\leq k$,
\begin{equation}
  C\left(\frac{\alpha_k P}{\left(1-\alpha_k\right)P + N_k}\right)<C\left(\frac{\alpha_k P}{\left(1-\alpha_k\right)P + N_i}\right)
\end{equation}
and therefore the condition in \ref{eq:k_cond_superpos} implies the condition in \ref{eq:i_cond_superpos}.\\
Knowing $U_k$, we can substract it from the received signal:
\begin{equation}
Y_i - U_k = U_1 + U_2 + ... +U_{k-1} + Z_i
\end{equation}
We can now repeat the exact same procedure to obtain all the messages up to $U_i$. At each step $l$, $i\leq l \leq k$, a new condition analogous to \ref{eq:i_cond_superpos} in the form of:
\begin{equation}
  R_l< C\left(\frac{\alpha_l P}{\left(\sum_{h=1}^{l-1}\alpha_h P+N_i\right)}\right)
\end{equation}
 must be satisfied to guarantee the correct decoding of $U_l$.
\item \textbf{@ receiver 1} proceeding iteratively, it is possible to decode all the messages up to message $u_1$. The last condition is
\begin{equation}
  R_1<C\left(\frac{\alpha_1 P}{N_1}\right)
\end{equation}
\end{itemize}
The set of conditions obtained during the decoding is summarized in table \ref{tab:conditions}.
\begin{table}[h!]
  \centering
  \begin{tabular}{c|c|c|c|c}
      Receiver & $R_k$ & $R_{k-1}$ &.. & $R_1$ \\
      \hline
      $k$ & $R_k<C\left(\frac{\alpha_k P}{\left(1-\alpha_k\right)P + N_k}\right)$ & - & .. & -\\
      $k-1$ & $R_k<C\left(\frac{\alpha_{k} P}{\left(1-\alpha_k\right)P + N_{k-1}}\right)$ & $R_{k-1}<C\left(\frac{\alpha_{k-1} P}{\left(1-\alpha_k-\alpha_{k-1}\right)P + N_{k-1}}\right)$ & .. & -\\
      . & . & . & . & . \\
      . & . & . & . & . \\
      . & . & . & . & . \\
      2 & $R_k<C\left(\frac{\alpha_{k} P}{\left(1-\alpha_k\right)P + N_{2}}\right)$ & $R_{k-1}<C\left(\frac{\alpha_{k-1} P}{\left(1-\alpha_k-\alpha_{k-1}\right)P + N_{2}}\right)$ & .. & - \\
      1 & $R_k<C\left(\frac{\alpha_{k} P}{\left(1-\alpha_k\right)P + N_{1}}\right)$ & $R_{k-1}<C\left(\frac{\alpha_{k-1} P}{\left(1-\alpha_k-\alpha_{k-1}\right)P + N_{1}}\right)$ & .. & $R_{1}<C\left(\frac{\alpha_{1} P}{N_{1}}\right)$\\
  \end{tabular}
  \caption{Conditions on the rate of each channel, at each receiver.}
  \label{tab:conditions}
\end{table}
As noticed before, the conditions can be simplified as in table \ref{tab:conditions_fin} considering that $N_k\geq N_{k-1}\geq ... \geq N_1$: the rate for channel $i$ is :
\begin{equation}
  R_i < C\left(\frac{\alpha_i P}{\sum_{h<i}\alpha_h P +N_i}\right)
\end{equation}
\begin{table}[h!]
\centering
  \begin{tabular}{c|c|c|c}
      $R_k$ & $R_{k-1}$ &.. & $R_1$ \\
      \hline
      $R_k<C\left(\frac{\alpha_k P}{\left(1-\alpha_k\right)P + N_k}\right)$ & $R_{k-1}<C\left(\frac{\alpha_{k-1} P}{\left(1-\alpha_k-\alpha_{k-1}\right)P + N_{k-1}}\right)$ & .. & $R_{1}<C\left(\frac{\alpha_{1} P}{N_{1}}\right)$\\
  \end{tabular}
  \caption{Summary of the conditions on the rate of each channel.}
  \label{tab:conditions_fin}
\end{table}
\subsection{Converse}
To prove the converse, we need to prove that for every code with rates $R_1, R_2, .., R_k$ there exists a sequence $\alpha_1, \alpha_2, ... , \alpha_k$, $\sum_{h=1}^k\alpha_k = 1$, such that
\begin{align}
  \centering
  \begin{split}
    & R_k<C\left(\frac{\alpha_k P}{\left(1-\alpha_k\right)P + N_k}\right),\\
    & R_i<C\left(\frac{\alpha_i P}{\sum_{h<i}\alpha_h P +N_i}\right),\text{ for } i = 2,..,k-1.\\
    & R_{1}<C\left(\frac{\alpha_{1} P}{N_{1}}\right),\\
  \end{split}
\end{align}

We will start from receiver $k$. Using Fano's inequality the converse condition can be written as
\begin{equation}
  nR_k\leq I(M_k;Y^n_k)
\end{equation}
Therefore to prove the converse we can just prove that
\begin{align}
  \begin{split}
    I(M_k;Y^n_k)\leq nC\left(\frac{\alpha_k P}{\left(1-\alpha_k\right)P + N_k}\right) = \frac{n}{2}\log\left(1+\frac{\alpha_k P}{\left(1-\alpha_k\right)P + N_k}\right)=\\
    =\frac{n}{2}\log\left(\frac{P+N_k}{\left(1-\alpha_k\right)P + N_k}\right)
  \end{split}
\end{align}
Let us consider the first term of the previous equation. Applying the properties of the mutual information, we get that
\begin{align}
  \begin{split}
    I(M_k;Y_k^n) = h(Y_k^n) - h(Y_k^n|M_k)
    \leq \frac{n}{2}\log(2\pi e (P+N_k))- h(Y_k^n|M_k)
    \label{eq:I_M_K}
  \end{split}
\end{align}
Consider the last term of equation \ref{eq:I_M_K}:
\begin{align}
  \begin{cases}
  h(Y_k^n|M_k)\geq h(Y_k^n|X^n,M_k) = h(Z^n_k) = \frac{n}{2}\log\left(2\pi e N_k\right) \\
  h(Y_k^n|M_k)\leq h(Y_k^n)\leq \frac{n}{2} \log(2\pi e (P+N_k))\\
  \end{cases}
  \\
  \implies \frac{n}{2}\log\left(2\pi e N_k\right)\leq h(Y_k^n|M_k) \leq \frac{n}{2} \log(2\pi e (P+N_k))
\end{align}
Therefore, there must exist a $\alpha_k \in [0,1]$ such that
\begin{equation}
  h(Y_k^n|M_k) = \frac{n}{2}\log(2\pi e ((1-\alpha_k) P+N_k))
  \label{eq:Y_kM_k}
\end{equation}
Substituing \ref{eq:Y_kM_k} in \ref{eq:I_M_K}, we obtain
\begin{align}
  \begin{split}
    I(M_k;Y_k^n) \leq \frac{n}{2}\log(2\pi e (P+N_k))- h(Y_k^n|M_k) =\\ =\frac{n}{2}\log(2\pi e (P+N_k))-\frac{n}{2}\log(2\pi e ((1-\alpha_k) P+N_k))\\
    = \frac{n}{2}\log(\frac{P+N_k}{(1-\alpha_k)P+N_k})
  \end{split}
  \label{eq:I_M_K}
\end{align}
and this demonstrates the converse for rate $R_k$.\\

A similar procedure can be applied to receiver $k-1$. The converse can be proved if we demonstrate that
\begin{align}
  \begin{split}
    nR_{k-1}\leq I(M_{k-1};Y_{k-1})\leq nC\left(\frac{\alpha_{k-1} P}{\left(1-\alpha_k-\alpha_{k-1}\right)P + N_{k-1}}\right) = \\
    = \frac{n}{2}\log\left(1+\frac{\alpha_{k-1} P}{\left(1-\alpha_k-\alpha_{k-1}\right)P + N_{k-1}}\right)\\
    = \frac{n}{2}\log\left(\frac{(1-\alpha_{k})P+N_{k-1} }{\left(1-\alpha_k-\alpha_{k-1}\right)P + N_{k-1}}\right)\\
    = \frac{n}{2}\log\left((1-\alpha_{k})P+N_{k-1}\right)-\frac{n}{2}\log\left(\left(1-\alpha_k-\alpha_{k-1}\right)P + N_{k-1}\right)
  \end{split}
\end{align}
First, we notice that for the properties of the mutual information and the independency of $M_k$, $M_{k-1}$:
\begin{equation}
  I(M_{k-1};Y^n_{k-1})\leq I(M_{k-1};Y^n_{k-1}|M_k)
\end{equation}
so for the converse we can just prove that
\begin{equation}
  I(M_{k-1};Y^n_{k-1}|M_k)\leq \frac{n}{2}\log\left((1-\alpha_{k})P+N_{k-1}\right)-\frac{n}{2}\log\left(\left(1-\alpha_k-\alpha_{k-1}\right)P + N_{k-1}\right)
  \label{eq:I_M-1_K-1}
\end{equation}
Again, as in \ref{eq:I_M_K}, we can express
\begin{equation}
  I(M_{k-1};Y^n_{k-1}|M_k) = h(Y_{k-1}^n|M_{k}) - h(Y_{k-1}^n|M_k,M_{k-1}).
  \label{eq:I_M_K-1_I_K-1|Mk}
\end{equation}
\begin{itemize}
  \item $\mathbf{h(Y_{k-1}^n|M_{k})}$. First, consider $h(Y_k^n|M_k)$. Recalling the definition of the degraded Gaussian BC, we know that
  \begin{equation}
      Y^n_k = Y^n_{k-1}+\tilde{Z}^n_k
  \end{equation}
  and applying the Entropy Power Inequality (EPI):
  \begin{align}
    \begin{split}
      2^{2h(Y^n_k|M_k)/n}\geq 2^{2h(Y^n_{k-1}|M_k)/n}+2^{2h(\tilde{Z}^n_{k}|M_k)/n}
    \end{split}
    \label{eq:EPI}
  \end{align}
  Since $\tilde{Z}^n_k$ and $M_k$ are independent, and $\tilde{Z}^n_k$ is distributed as $\mathcal{N}(0,\tilde{N}_k)$
  \begin{align}
    \begin{split}
      h(\tilde{Z}^n_{k}|M_k) = h(\tilde{Z}^n_{k}) = \frac{n}{2}\log\left(2\pi e \tilde{N}_k\right).
    \end{split}
  \end{align}
  Using this result in the \ref{eq:EPI}, and taking the logarithm in both sides,
  \begin{align}
    \begin{split}
      2h(Y^n_k|M_k)/n\geq \log\left(2^{2h(Y^n_{k-1}|M_k)/n}+2^{\log\left(2\pi e \tilde{N}_k\right)}\right) \\
      = \log\left(2^{2h(Y^n_{k-1}|M_k)/n}+2\pi e \tilde{N}_k\right)\\
      \implies h(Y^n_k|M_k)\geq \frac{n}{2}\log\left(2^{2h(Y^n_{k-1}|M_k)/n}+2\pi e \tilde{N}_k\right)
    \end{split}
  \end{align}
  Now, we go back to decoder $k$ and use again the result in \ref{eq:Y_kM_k}, from the above equation we obtain that
  \begin{equation}
    h(Y_k^n|M_k) = \frac{n}{2}\log(2\pi e ((1-\alpha_k) P+N_k))\geq \frac{n}{2}\log\left(2^{2h(Y^n_{k-1}|M_k)/n}+2\pi e \tilde{N}_k\right)
  \end{equation}
  Since the logarithm is monotonically increasing, we can move the relation to the arguments:
  \begin{align}
    \begin{split}
      2\pi e ((1-\alpha_k) P+N_k))\geq (2^{2h(Y^n_{k-1}|M_k)/n}+2\pi e \tilde{N}_k\\
      \implies
      2\pi e ((1-\alpha_k) P+N_k-\tilde{N}_k)\geq 2^{2h(Y^n_{k-1}|M_k)/n}\\
      \implies
      \log(2\pi e ((1-\alpha_k) P+N_k-\tilde{N}_k))\geq 2h(Y^n_{k-1}|M_k)/n\\
      \implies
      \frac{n}{2}\log(2\pi e ((1-\alpha_k) P+N_k-\tilde{N}_k))\geq h(Y^n_{k-1}|M_k)
    \end{split}
    \label{eq:Yk-1_Mk}
  \end{align}
  For the structure of the degraded Gaussian BC, $N_k-\tilde{N}_k=N_{k-1}$ and \ref{eq:Yk-1_Mk} becomes
  \begin{align}
    \frac{n}{2}\log(2\pi e ((1-\alpha_k) P+N_{k-1}))\geq h(Y^n_{k-1}|M_k)
    \label{eq:Yk-1|Mk}
  \end{align}
  that is the first term in \ref{eq:I_M-1_K-1}, so:
  \begin{align}
    \begin{split}
        I(M_{k-1};Y^n_{k-1}|M_k) = h(Y_{k-1}^n|M_{k}) - h(Y_{k-1}^n|M_k,M_{k-1})\leq\\
        \leq \frac{n}{2}\log(2\pi e ((1-\alpha_k) P+N_{k-1})) - h(Y_{k-1}^n|M_k,M_{k-1})
    \end{split}
    \label{eq:final_1}
  \end{align}
  \item $\mathbf{h(Y_{k-1}^n|M_k,M_{k-1})}$. Consider now the second term in \ref{eq:I_M_K-1_I_K-1|Mk}. We can apply the same reasoning as in \ref{eq:Y_kM_k} and set the sequence of disequations:
  \begin{align}
    \begin{cases}
      h(Y_{k-1}|M_{k-1},M_{k})\geq h(Y_{k-1}|M_{k-1},M_{k},X) =h(Z_{k-1}) = \frac{n}{2}\log(2\pi e N_{k-1}) \\
      h(Y_{k-1}|M_{k-1},M_{k})\leq h(Y_{k-1}|M_{k})\overset{(\ref{eq:Yk-1|Mk})}{\leq} \frac{n}{2}\log(2\pi e ((1-\alpha_k) P+N_{k-1}))
    \end{cases}
  \end{align}
  from which
  \begin{equation}
    \frac{n}{2}\log(2\pi e N_{k-1})\leq h(Y_{k-1}|M_{k-1},M_{k})\leq \frac{n}{2}\log(2\pi e ((1-\alpha_k) P+N_{k-1}))
  \end{equation}
  and therefore there must exist $\alpha_{k-1}$ such that
  \begin{equation}
    h(Y_{k-1}|M_{k-1},M_{k}) = \frac{n}{2}\log(2\pi e ((1-\alpha_k-\alpha_{k-1}) P+N_{k-1})).
    \label{eq:final_2}
  \end{equation}
  The parameter $\alpha_{k-1}$ is positive and must vary in such a way so that
  \begin{equation*}
    0\leq(1-\alpha_k-\alpha_{k-1})\leq 1
    \implies 0\leq\alpha_{k-1}\leq 1-\alpha_k
  \end{equation*}
\end{itemize}
Combining the previous points:
\begin{align}
  \begin{split}
    I(M_{k-1};Y^n_{k-1}|M_k) = h(Y_{k-1}^n|M_{k}) - h(Y_{k-1}^n|M_k,M_{k-1})\leq\\
    \overset{(\ref{eq:final_1})}{\leq} \frac{n}{2}\log(2\pi e ((1-\alpha_k) P+N_{k-1})) - h(Y_{k-1}^n|M_k,M_{k-1}) =\\
    \overset{(\ref{eq:final_2})}{=}\frac{n}{2}\log(2\pi e ((1-\alpha_k) P+N_{k-1}))-\frac{n}{2}\log(2\pi e ((1-\alpha_k-\alpha_{k-1}) P+N_{k-1}))=\\
    = \frac{n}{2}\log\left(\frac{((1-\alpha_k) P+N_{k-1}}{(1-\alpha_k-\alpha_{k-1}) P+N_{k-1}}\right)
  \end{split}
  \label{eq:final_k-1}
\end{align}
Now the procedure can be iterated for every $1\leq i\leq k-1$ in the same way. At every step $i$, the condition on $\alpha_i$ is
\begin{equation}
  0\leq \alpha_i \leq 1-\sum_{h>i}\alpha_h
\end{equation}
up to the first receiver, where $\alpha_1 = 1-\sum_{h\neq 1}\alpha_h$ so that the power constrain is also satisfied:
\begin{equation}
\sum_{h = 1}^k \alpha_h = 1.
\end{equation}

% and considering the fact that $M_i$ is independent of any other $M_h$ with $h\leq i$, for the properties of the mutual information
% \begin{equation}
%   nR_i\leq I(M_i;Y^n_i)\leq I(M_i;Y^n_i|M_h).
% \end{equation}
% Therefore to prove the converse we can just prove that
% \begin{equation}
%   I(M_i;Y^n_i|M_h)\leq nC\left(\frac{\alpha_i P}{\sum_{h<i}\alpha_h P +N_i}\right).
% \end{equation}
% Consider $I(M_i;Y_i^n|M_{h<i})$ where $M_{h<i} = M_1,...,M_{i-1}$. Applying the properties of the mutual information, we get that
% \begin{align}
%   \begin{split}
%     I(M_i;Y_i^n|M_{h<i}) = h(Y_i^n|M_{h<i}) - h(Y_i^n|M_i,M_{h<i}) = h(Y_i^n|M_{h<i}) - h(Y_i^n|M_{h\leq i})\\
%     \leq \frac{n}{2}\log(2\pi e (P+N_i))- h(Y_i^n|M_{h\leq i})
%     \label{eq:I_M_Y}
%   \end{split}
% \end{align}
% Let us analyse the first term:
% \begin{align*}
%   \frac{n}{2}\log(2\pi e N_i) = h(Y^n_i|M_h) \leq h(Y^n_i|M_{h\leq i})\leq \frac{n}{2}\log(2\pi e (P+N_i)).
% \end{align*}
% Then, there exists $\alpha_i \in [0,1]$ such that
% \begin{equation}
%   h(Y_i^n|M_{h\leq i}) = \frac{n}{2} \log(2\pi e (\alpha_i P+N_i)).
%   \label{eq:h_Y}
% \end{equation}
% Applying \ref{eq:h_Y} to \ref{eq:I_M_Y}, we obtain
% \begin{equation}
%   I(M;Y_i^n|M_{h<i})\leq \frac{n}{2}\log\left(\frac{P+N_i}{\alpha_i P + N_i}\right)
% \end{equation}
