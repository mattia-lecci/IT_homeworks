%\section{Problem 3.5}
\mysection{3.5}{Problem 3.5}

\numberwithin{equation}{subsection}
\subsection{MLD - minimum distance equivalence}
Let's define
%
\begin{equation}
N(m) \triangleq \left|\{ i: y_i=x_i(m) \} \right|
\end{equation}
%
Note that it is strictly related to to the hamming distance, in fact $d(x^n(m),y^n) = n-N(m)$.

In the case of a memoryless channel, the MLD works as follows:
%
\begin{equation}
\hat{m} = \argmax_m \prod_{i=1}^{n} p_{Y|X}(y_i|x_i(m)) = \argmax_m (1-p)^{N(m)} p^{n-N(m)}
\end{equation}
%
where the last equality comes from the fact that the Binary Symmetric Channel (BSC) only cares about whether a symbol is correctly or incorrectly transmitted. Assuming that $x^n(m)$ is sent and $y^n$ is received, there will be $N(m)$ correctly transmitted bits (each w.p. $(1-p)$ and $n-N(m)$ errors (each w.p. $p$).

Using the natural logarithm (which is a strictly monotonically increasing function) we obtain
%
\begin{align}
\begin{split}
\hat{m} =& \argmax_m N(m)\log(1-p) + (n-N(m)) \log p\\
\eqtext{(a)}& \argmax_m N(m)\log(1-p) -N(m) \log p\\
=& \argmax_m N(m)\log\left( \frac{1-p}{p} \right)\\
\eqtext{(b)}& \argmax_m N(m)\\
\eqtext{(c)}& \argmin_m n-N(m)\\
=& \argmin_m d( x^n(m),y^n)
\end{split}
\end{align}
%
where in $(a)$ we noted that the factor $n \log p$ doesn't depend on $m$, hence it can be neglected, in $(b)$ again we neglect the constant positive term (by assumption $p<\frac{1}{2}$), in $(c)$ we apply a strictly monotonically decreasing function ($x \rightarrow n-x$ with a given $n$), hence we need to search for a minimum instead of a maximum. The last equality has already been discussed at the beginning of the section.

\subsection{Error bound}

Now we want to bound the error probability of MLD as follows.

\begin{equation}
	\begin{aligned}
	P_e^{(n)} & =P\{\hat{M}\neq 1 | M = 1\} \\
	& = P \{ \exists \ m \neq 1 : \x{m} < \x{1} | M=1 \}\\
	& \leq P \{ \x{1} >n(p+\epsilon)|M=1\}+(2^{nR}-1)P \{ \x{2} \leq n(p+\epsilon)|M=1\}
	\end{aligned}
	\label{eq:3.5bound}
\end{equation}

We notiche that we are allowed to write $P_e^{(n)}  =P \{\hat{M}\neq 1 | M = 1\}$ since we are averaging the error probability over all possible \textit{codebook}. Given $d \geq 0$ we define the following events.

\begin{gather*}
	A = \Big \{ \x{1}>d | M = 1\Big \} \\
	B = \Big \{ \forall \  m \neq 1 : \x{m}>d | M=1 \Big \} \\
 	C = \Big\{ \exists \ m \neq 1 : \x{m} < \x{1} | M=1 \Big \}
\end{gather*}

We notice that $C$ coincide to the error event. This imply that $P_e^{(n)}=P(C)$. In addiction we notice that if the events $B$ and $C$ are both verified, also the event $A$ is verified. This imply that $A$ is a supset of the intersection between $B$ and $C$ $\forall$ $d \geq 0$.

\begin{equation}
		A \supseteq B \cap C \quad \forall \  d\geq0
	\label{eq:3.5condition}
\end{equation}

Then we can write what follows.

\begin{equation}
	\begin{gathered}
		\Rightarrow P \{ A \} \geq P\{B\}+P\{C \} - P \{ B \cup C \} \\
		\Rightarrow P \{ C \} \leq P \{ A \} + P \{ B \cup C \} - P\{B \}
	\end{gathered}
\end{equation}

Now we call $\overline{B}$ the complementary event of  $B$.

\begin{align*}
	\overline{B} = \Big \{ \exists \  m \neq 1 : \x{m} \leq d | M = 1 \Big \}
\end{align*}

 It is obvious that $P(\overline{B})=1-P(B)$. Then we can write what follows.

 \begin{equation}
 	\begin{gathered}
		\Rightarrow P \{ C \} \leq P \{ A \} + P \{ B \cup C \} -(1-P\{\overline{B} \})  \\
		\Rightarrow P \{ C \} \leq P \{ A \} + P\{\overline{B} \}-(1-P \{ B \cup C \} )
 	\end{gathered}
 \end{equation}

Now we notice that $1-P \{ B \cup C \}$ is always greater or equal to $0$ since $P \{ B \cup C \}$ is a probability. Then $-(1-P \{ B \cup C \})$ is always lower or equal to $0$ and we can write what follows.

\begin{equation}
 \begin{gathered}
	 \Rightarrow P \{ C \} \leq P \{ A \} + P\{\overline{B} \}
 \end{gathered}
\label{eq:3.5diseq}
\end{equation}

Now we notice that $\overline{B}$ can be defined as an union of the events $\overline{B}_m$.

\begin{equation}
		\overline{B} = \bigcup _{m=2}^{2^{nR}} \overline{B}_m = \bigcup _{m=2}^{2^{nR}} \Big \{ \x{m} \leq d | M = 1\Big \}
		\label{eq:3.5Bunion}
\end{equation}

Given \eqref{eq:3.5Bunion} we can bound $P\{ \overline{B}\}$ as follows.

\begin{equation}
	P\{\overline{B} \} \leq \sum_{m=2}^{2^{nR}} P \{ \overline{B}_m \} = \sum_{m=2}^{2^{nR}} P \{ \x{m} \leq d | M = 1 \}
	\label{eq:3.5Bsum}
\end{equation}

Since we are using \textit{random coding} the following statemente is always true.

\begin{equation}
	P \{ \x{m} \leq d | M = 1 \} = P \{ \x{2} \leq d | M = 1  \} \ \forall \ m \neq 1
\end{equation}

Then we can write \eqref{eq:3.5Bsum} as follows.

\begin{equation}
	P\{\overline{B} \} \leq (2^{nR}-1) P \{ \x{2} \leq d | M = 1 \}
	\label{eq:3.5Bbound}
\end{equation}

We remeber that $P_e^{(n)}=P(C)$. Therefore given \eqref{eq:3.5Bbound} and \eqref{eq:3.5diseq} we conclude that the following statement is true $\forall \  d \geq 0$.

\begin{equation}
	\begin{gathered}
		P_e^{(n)} \leq
		P \{ \x{1}>d | M = 1 \} + (2^{nR}-1) P \{ \x{2} \leq d | M = 1 \}
	\end{gathered}
\end{equation}

Now we assign to $d$ the value $n(p+\epsilon)$, which is always greater or equal to zero $\forall \  \epsilon \geq 0$. Finally we get the request bound.

\begin{equation}
	\begin{gathered}
		P_e^{(n)} \leq
		P \{ \x{1}> n(p+\epsilon) | M = 1 \} + (2^{nR}-1) P \{ \x{2} \leq n(p+\epsilon) | M = 1 \}
	\end{gathered}
\end{equation}

\subsection{Achievability result}

First, we want to show that
%
\begin{equation}\label{eq:3.5c_goal}
\Pr[d(X^n(1),Y^n)>n(p+\varepsilon) | M=1] \xrightarrow{n\rightarrow \infty} 0
\end{equation}

Recalling the Weak Law of Large Number for iid random variables
%
\begin{equation}
\lim_{n \rightarrow \infty} \Pr \qty( \qty|\frac{1}{n} \sum_{i=1}^{n} X_i - E[X]|> \varepsilon ) = 0
\end{equation}
%
we see that it's closely related to our case of study. In fact consider
%
\begin{equation}
E[d(X^n(1),Y^n)] = E\qty[ \sum_{i=1}^{n} \mathds{1}[Y_i(1) \neq Y_i] ] = \sum_{i=1}^n E[ \mathds{1}[Y_i(1) \neq Y_i] ] = \sum_{i=1}^n p = np
\end{equation}

From here and from Eq.~\eqref{eq:3.5c_goal} we cam rearrange terms
%
\begin{equation}
\Pr \qty[ \frac{1}{n} d(X^n(1),Y^n) - p > \varepsilon |M=1] \leq \Pr \qty[ \qty| \frac{1}{n} d(X^n(1),Y^n) - p| > \varepsilon] \xrightarrow{n\rightarrow \infty} 0
\end{equation}
%
where the inequality comes from the fact that, considering $X$ a generic random variable and $a>0$ a number, the event $\{|X|>a\}$ can be written as $\{X>a\} \cup \{X<-a\}$ (which are clearly disjoint events). Hence $\Pr[|X|>a] = \Pr[X>a] + \Pr[X<-a] \geq \Pr[X>a]$ (since probabilities are non-negative quantities).

This proves the first part of the problem.

Now, considering the given bound, we can write that
%
\begin{equation}
\begin{split}
P_e^{(n)} \leq& \Pr[d(X^n(1),Y^n)>n(p+\varepsilon) | M=1] +\\
	&(2^{nR}-1) \Pr[d(X^n(2),Y^n) \leq n(p+\varepsilon) | M=1] \\
\leq& \Pr[d(X^n(1),Y^n)>n(p+\varepsilon) | M=1] + (2^{nR}-1)2^{-n(1-H(p+\varepsilon))}\\
\leq& \Pr[d(X^n(1),Y^n)>n(p+\varepsilon) | M=1] + 2^{-n(1-H(p+\varepsilon)-R)}
\xrightarrow{n\rightarrow \infty} 0
\end{split}
\end{equation}
%
if $R<1-H(p+\varepsilon)$. Since we know that the bound holds $\forall \varepsilon>0$ we obtain the result for any $R<C=1-H(p+\varepsilon)$.

\numberwithin{equation}{section}
