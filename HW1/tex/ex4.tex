%\section*{Problem 3.14}
\mysection{3.14}{Problem 3.14}

\newcommand{\typical}{\ensuremath{\mathcal{T}_\varepsilon^{(n)}}}
\newcommand{\typicalLong}{\ensuremath{\mathcal{T}_\varepsilon^{(n)}(X^n,Y^n)}}

We decided to use the following coding/decoding scheme: the coder simply encodes the message $M$ into the codeword $X^n(M)$ like in the normal case seen in class. The decoder, instead, looks at all possible codewords $X^n(m) \; \forall m\in \mathcal{M}$, looks whether or not they are typical and creates $\mathcal{L}(Y^n) = \typicalLong$ if $|\typical| \leq 2^{nL}$ or $\mathcal{L}(Y^n)=\emptyset$ if $|\typical| > 2^{nL}$.

\numberwithin{equation}{subsection}
\subsection{Achievability}
Using the coder/decoder description given at the beginning of the section, the error probability can be written as
%
\begin{align}
\begin{split}
P_e^{(n)}=&E_\mathcal{C}[P_{e,\mathcal{C}}^n] = \Pr( M\notin \mathcal{L}(Y^n) |M=1 )\\
=& \Pr( \{(X^n(1),Y^n)\notin\typical| M=1\} \cup \{ |\typical|>2^{nL} \}|M=1)\\
\leq& \Pr((X^n(1),Y^n)\notin\typical|M=1) + \Pr(|\typical|>2^{nL}|M=1)\\
\triangleq& \Pr(\mathcal{E}_1) + \Pr(\mathcal{E}_2)
\end{split}
\end{align}

As seen in class, by the conditional typicality lemma $P(\mathcal{E}_1) \xrightarrow{n\rightarrow\infty}0$. All the effort, once again, will be focused on creating appropriate bounds for $\Pr(\mathcal{E}_2)$ as follows
%
\begin{align} \label{eq:3.14_E2}
\begin{split}
\Pr(\mathcal{E}_2) =& \Pr(|\typicalLong|>2^{nL}|M=1)\\
 \leq& \Pr(|\typicalLong| \geq 2^{nL}|M=1)\\
\leq& \frac{E \qty[\qty|\typicalLong| \,|M=1]}{2^{nL}}
\end{split}
\end{align}
%
where the last inequality follows from Markov's Inequality.

Now, we can bound the mean cardinality of the typicality set as follows:
%
\begin{align} \label{eq:3.14_expectation_bound}
\begin{split}
E \qty[\qty|\typicalLong| \,|M=1] =& E\qty[ \sum_{m=1}^{2^{nR}} \indicator((X^n(m),Y^n) \in \typical) |M=1 ]\\
=& \sum_{m=1}^{2^{nR}} E[\indicator((X^n(m),Y^n)\in \typical |M=1)]\\
=& \sum_{m=1}^{2^{nR}} \Pr((X^n(m),Y^n)\in \typical |M=1)\\
\leq& \Pr((X^n(1),Y^n)\in \typical |M=1)\\
& + \sum_{m=2}^{2^{nR}} \Pr((X^n(m),Y^n)\in \typical |M=1)\\
\stackrel{(a)}{\leq}& 1 + (2^{nR}-1)2^{-n(I(X;Y)-\delta(\varepsilon))}\\
\leq& 1 + 2^{-n(C-R - \delta(\varepsilon))}
\end{split}
\end{align}
%
where in $(a)$ we separated the probability of the first codeword to be typical with $Y^n$ since it has to be treated differently from the others (given the fact that actually $M=1$ was sent, so their probability of being jointly typical is higher than the rest) and we used the bound found in class for the rest of the codewords.

Now, joining Eqs.~\eqref{eq:3.14_expectation_bound},~\eqref{eq:3.14_E2} we obtain
%
\begin{equation}
P_e^{(n)} = \Pr(\mathcal{E}_1) + \Pr(\mathcal{E}_2) \leq \Pr(\mathcal{E}_1) +  2^{-nL} + 2^{-n(C+L-R - \delta(\varepsilon))}
\end{equation}
%
which tends to zero if $R<C+L$.

\subsection{Converse}
We will start showing the modified Fano's Inequality.

We are interested in the error probability defined as $P_e^{(n)} = \Pr(M \notin \mathcal{L}(Y^n))$. For this, similarly to the proof of Fano's Inequality seen in class, we define the random variable $\mathcal{E}$ such that $\mathcal{E}=0$ if $M \in \mathcal{L}$, $\mathcal{E}=1$ if $M \notin \mathcal{L}$. This implies that $E[\mathcal{E}] = P_e^{(n)}$ and $H(\mathcal{E}) = H(P_e^{(n)})$. Again, following the same procedure we obtain
%
\begin{equation}
H(M|\mathcal{L}) = H(\mathcal{E}|\mathcal{L})
+H(M|\mathcal{L},\mathcal{E})
-H(\mathcal{E}|M,\mathcal{L})
\end{equation}

Now, clearly $H(\mathcal{E}|M,\mathcal{L})=0$ since once we know the message $M$ and the list $\mathcal{L}$, we deterministically know the value of $\mathcal{E}$. Furthermore, $H(\mathcal{E}|\mathcal{L}) \leq H(\mathcal{E}) = H(P_e^{(n)})$ due to the conditional entropy property. Finally, we can write
%
\begin{equation}
H(M|\mathcal{E},\mathcal{L}) = P_e^{(n)} H(M|\mathcal{E}=1,\mathcal{L}) + (1-P_e^{(n)})H(M|\mathcal{E}=0,\mathcal{L})
\end{equation}
%
Now, if $\mathcal{E}=0$ it means that $M \in \mathcal{L}$, thus knowing both this and $\mathcal{L}$ means that $M$ could be any of the messages in the list, i.e. $H(M|\mathcal{L},\mathcal{E}=0) \leq \log_2|\mathcal{L}|$ (equal if the message is uniformly distributed). Similarly, if $M \notin \mathcal{L}$ it must be one of the other $|\mathcal{M}|-|\mathcal{L}|$ messages, i.e. $H(M|\mathcal{L},M\notin \mathcal{L}) \leq \log_2(|\mathcal{M}|-|\mathcal{L}|)$.

Bringing all together we get
%
\begin{align}
\begin{split}
H(M|\mathcal{L}) \leq & H(P_e^{(n)}) + P_e^{(n)}\log_2(|\mathcal{M}|-|\mathcal{L}|) + (1-P_e^{(n)})\log_2|\mathcal{L}|\\
=& H(P_e^{(n)}) + \log_2|\mathcal{L}| + P_e^{(n)}\log_2\qty(\frac{|\mathcal{M}|-|\mathcal{L}|}{|\mathcal{L}|})\\
=& H(P_e^{(n)}) + \log_2|\mathcal{L}| + P_e^{(n)}\log_2\qty(\frac{|\mathcal{M}|}{|\mathcal{L}|} - 1)\\
\leq& 1 + \log_2|\mathcal{L}| + P_e^{(n)}\log_2\frac{|\mathcal{M}|}{|\mathcal{L}|}
\end{split}
\end{align}

In our case we have $\mathcal{M}=2^{nR}$ and $1\leq |\mathcal{L}| \leq 2^{nL}$, thus we can upper bound $\log_2|\mathcal{L}| \leq nL$ and $\log_2(|\mathcal{M}|/|\mathcal{L}|) \leq nR$, thus yielding
%
\begin{equation}
H(M|\mathcal{L}) \leq 1+nL+P_e^{(n)} \, nR = nL + n\qty(\frac{1}{n} + P_e^{(n)} R) = nL + n\varepsilon_n
\end{equation}

Now the proof of the converse is exactly the same as the one seen in class, using this modified version of Fano's Inequality:
%
\begin{align}
\begin{split}
nR =& H(M)\\
=&  I(M;Y^n) + H(M|Y^n)\\
\stackrel{(a)}{\leq}& I(M;Y^n) + nL + n\varepsilon_n\\
=& \sum_{i=1}^n I(M;Y_i|Y^{i-1}) + nL+n\varepsilon_n\\
\stackrel{(b)}{\leq}& \sum_{i=1}^n I(M,Y^{i-1};Y_i) + nL+n\varepsilon_n\\
\stackrel{(c)}{=}& \sum_{i=1}^n I(M,X_i,Y^{i-1};Y_i) + nL+n\varepsilon_n\\
\stackrel{(d)}{=}& \sum_{i=1}^n I(X_i;Y_i) + nL+n\varepsilon_n\\
\leq& nC + nL+n\varepsilon_n
\end{split}
\end{align}
%
thus $R\leq C+L$ since $\varepsilon_n \xrightarrow{n\rightarrow\infty} 0$. In $(a)$ we used the fact that $\mathcal{L} \independent M |Y^n$ (or in Markov chain notation $M\rightarrow Y^n\rightarrow \mathcal{L}$), thus $H(M|Y^n)\leq H(M|\mathcal{L}) \leq nL + n\varepsilon_n$. In $(b)$ we used the conditioning rule $I(M,Y^{i-1};Y_i) = I(Y^{i-1};Y_i) + I(M;Y_i|Y^{i-1}) \geq I(M;Y_i|Y^{i-1})$. In $(c)$ we know that $X_i$ is a deterministic function of $M$ so it doesn't add or subtract any information. In $(d)$ we exploit the memoryless property of the channel and the conditional independence $Y_i \independent (M,Y^{i-1})|X_i$.

\numberwithin{equation}{section}
